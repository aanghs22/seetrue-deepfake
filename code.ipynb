{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeeTrue Competition: Code for DeepFakes\n",
    "_Code Template from DSTA_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ##**Connect to a GPU Runtime** \n",
    "#@markdown GPUs speed up the training of deep learning models by enabling parallel computations. Connect to a GPU runtime to speed up the running of this notebook.\n",
    "\n",
    "def display_gpu():\n",
    "  \"\"\"\n",
    "  Print the GPU runtime that you are connected to.\n",
    "  \"\"\"\n",
    "  return !nvidia-smi\n",
    "\n",
    "display_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ##**Install required libraries**\n",
    "\n",
    "models_dir = \"/content/models\"\n",
    "images_dir = \"/content/images\"\n",
    "results_dir = \"/content/results\"\n",
    "character_img = \"/content/character_img.png\"\n",
    "trimmed_character_img = \"/content/trimmed_character_img.png\"\n",
    "driving_video = \"/content/driving_video.mp4\"\n",
    "animated_video_no_audio = f\"{results_dir}/animated_video_no_audio.mp4\"\n",
    "animated_video = f\"{results_dir}/animated_video.mp4\"\n",
    "final_video = f\"{results_dir}/final_video.mp4\"\n",
    " \n",
    "\n",
    "%cd \"/content\"  \n",
    "print(\"Downloading Packages...\")\n",
    "\n",
    "from pathlib import Path\n",
    "Path(models_dir).mkdir(parents=True,exist_ok=True)\n",
    "Path(images_dir).mkdir(parents=True,exist_ok=True)\n",
    "Path(results_dir).mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# First-Order-Model\n",
    "!git clone \"https://github.com/seetrueinfo/first-order-model.git\"\n",
    "!wget -nc \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt2.xml\" -O \"/content/first-order-model/haarcascade_frontalface_alt2.xml\" &> /dev/null\n",
    "\n",
    "# JojoGAN\n",
    "!git clone https://github.com/seetrueinfo/JoJoGAN.git\n",
    "!wget -nc https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "!sudo unzip -n ninja-linux.zip -d /usr/local/bin/\n",
    "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
    "!rm \"/content/ninja-linux.zip\"\n",
    "\n",
    "print(\"Installing required libraries\")\n",
    "!pip install -r first-order-model/requirements.txt &> /dev/null \n",
    "!pip install ffmpeg scikit-video &> /dev/null \n",
    "\n",
    "print(\"Succesfully Finished Installing Libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ##**Import libraries**\n",
    "#@markdown A program must import libraries before using them. \n",
    "\n",
    "#@markdown In this cell, we import all libraries to be used and provide some helper functions that will be used throughout this notebook.\n",
    "#@markdown This may take a while.\n",
    "\n",
    "print(\"Loading Libraries and helper functions...\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/JoJoGAN')\n",
    "import shutil \n",
    "shutil.rmtree('/content/sample_data',ignore_errors=True)\n",
    "from util import *  \n",
    "from torchvision import utils \n",
    "import torch \n",
    "from PIL import Image  \n",
    "from model import *\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from io import StringIO\n",
    "from IPython import get_ipython\n",
    "from scipy.io import wavfile\n",
    "import skvideo.io  \n",
    "from google.colab import files\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "# First-order-model\n",
    "import imageio\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from skimage.transform import resize\n",
    "from numpy.core import memmap\n",
    "import imutils\n",
    "\n",
    "# Record and display video \n",
    "from google.colab.output import eval_js\n",
    "from IPython.display import Javascript, display, HTML, Image as display_image\n",
    "from base64 import b64decode, b64encode\n",
    "\n",
    "\n",
    "def getLocalFiles():\n",
    "    \"\"\"\n",
    "    Helper function to upload files from your laptop to this Colab notebook.\n",
    "    Note that files are not persistent! (i.e. if you restart runtime, all your\n",
    "    uploads will be lost)\n",
    "    \"\"\"\n",
    "    uploaded = files.upload()\n",
    "    filename = next(iter(uploaded))\n",
    "    return filename\n",
    "\n",
    "\n",
    "def record_video(filename):\n",
    "    \"\"\"\n",
    "    Helper function to record video in Colab.\n",
    "    \"\"\"\n",
    "    js=Javascript(\"\"\"\n",
    "        async function recordVideo() {\n",
    "        const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
    "        const div = document.createElement('div');\n",
    "        const capture = document.createElement('button');\n",
    "        const stopCapture = document.createElement(\"button\");\n",
    "        \n",
    "        capture.textContent = \"Start Recording\";\n",
    "        capture.style.background = \"orange\";\n",
    "        capture.style.color = \"white\";\n",
    "\n",
    "        stopCapture.textContent = \"Stop Recording\";\n",
    "        stopCapture.style.background = \"red\";\n",
    "        stopCapture.style.color = \"white\";\n",
    "        div.appendChild(capture);\n",
    "\n",
    "        const video = document.createElement('video');\n",
    "        const recordingVid = document.createElement(\"video\");\n",
    "        video.style.display = 'block';\n",
    "\n",
    "        const stream = await navigator.mediaDevices.getUserMedia({audio:true, video: true});\n",
    "        \n",
    "        let recorder = new MediaRecorder(stream, options);\n",
    "        document.body.appendChild(div);\n",
    "        div.appendChild(video);\n",
    "\n",
    "        video.srcObject = stream;\n",
    "        video.muted = true;\n",
    "\n",
    "        await video.play();\n",
    "\n",
    "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "        await new Promise((resolve) => {\n",
    "            capture.onclick = resolve;\n",
    "        });\n",
    "        recorder.start();\n",
    "        capture.replaceWith(stopCapture);\n",
    "\n",
    "        await new Promise((resolve) => stopCapture.onclick = resolve);\n",
    "        recorder.stop();\n",
    "        let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
    "        let arrBuff = await recData.data.arrayBuffer();\n",
    "        \n",
    "        // stop the stream and remove the video element\n",
    "        stream.getVideoTracks()[0].stop();\n",
    "        div.remove();\n",
    "\n",
    "        let binaryString = \"\";\n",
    "        let bytes = new Uint8Array(arrBuff);\n",
    "        bytes.forEach((byte) => {\n",
    "            binaryString += String.fromCharCode(byte);\n",
    "        })\n",
    "        return btoa(binaryString);\n",
    "        }\n",
    "    \"\"\"\n",
    "    )\n",
    "    try:\n",
    "        display(js)\n",
    "        data=eval_js('recordVideo({})')\n",
    "        binary=b64decode(data)\n",
    "        with open(filename,\"wb\") as video_file:\n",
    "            video_file.write(binary)\n",
    "            print(f\"Finished recording video at:{filename}\")\n",
    "    except Exception as err:\n",
    "        print(str(err))\n",
    "\n",
    "\n",
    "def display_video(vid_filename):\n",
    "    \"\"\"\n",
    "    Display the video in colab. \n",
    "    \"\"\"\n",
    "    # display result\n",
    "    mp4 = open(vid_filename,'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(\"\"\"\n",
    "    <video width=400 controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ##**Download Models and images** \n",
    "\n",
    "drive_ids = {  \n",
    "    \"dlibshape_predictor_68_face_landmarks.dat\":\"https://www.dropbox.com/s/2a0arq1px988ry4/dlibshape_predictor_68_face_landmarks.dat?dl=0\",\n",
    "    \"bella_einstein.mp4\":\"https://www.dropbox.com/s/ugynax1n3vt049w/bella_einstein.mp4?dl=0\",\n",
    "    \"driving_video.mp4\":\"https://www.dropbox.com/s/j2xes309icr1kor/driving_video.mp4?dl=0\", \n",
    "    \"logo_small.png\":\"https://www.dropbox.com/s/pkczqq59s5vi1kt/logo_small.png?dl=0\", \n",
    "    \"einstein.png\": \"https://www.dropbox.com/s/eitp4rlh516b7lx/einstein.png?dl=0\",\n",
    "    \"lincoln.png\":\"https://www.dropbox.com/s/efr8y186cwbuka0/lincoln.png?dl=0\",\n",
    "    \"nietzsche.png\":\"https://www.dropbox.com/s/zbxyuwvooy3oh90/nietzsche.png?dl=0\",\n",
    "    \"sokrates.png\":\"https://www.dropbox.com/s/6ehlgqeacs2a2nt/sokrates.png?dl=0\",\n",
    "    \"van_gogh.png\":\"https://www.dropbox.com/s/i4jqk3pu8u8gxj1/van_gogh.png?dl=0\",\n",
    "    \"mona_lisa.png\": \"https://www.dropbox.com/s/ewkmhyl6o8znp8w/mona_lisa.png?dl=0\",\n",
    "    \"vox-cpk.pth.tar\" : \"https://www.dropbox.com/s/ih36quf8vlwagxh/vox-cpk.pth.tar?dl=0\",\n",
    "    \"stylegan2-ffhq-config-f.pt\": \"https://www.dropbox.com/s/4lhdbzmjanvaei3/stylegan2-ffhq-config-f.pt?dl=0\",\n",
    "    \"e4e_ffhq_encode.pt\": \"https://www.dropbox.com/s/3u1ewa19m3ovvdj/e4e_ffhq_encode.pt?dl=0\", \n",
    "    \"arcane_caitlyn.pt\": \"https://www.dropbox.com/s/3nzkazv9lmssl4z/arcane_caitlyn.pt?dl=0\",\n",
    "    \"arcane_caitlyn_preserve_color.pt\": \"https://www.dropbox.com/s/rl0726jpw1fwiav/arcane_caitlyn_preserve_color.pt?dl=0\",\n",
    "    \"arcane_jinx_preserve_color.pt\": \"https://www.dropbox.com/s/dqw421ahlv1aac9/arcane_jinx_preserve_color.pt?dl=0\",\n",
    "    \"arcane_jinx.pt\": \"https://www.dropbox.com/s/ri7u36vwith0k3k/arcane_jinx.pt?dl=0\",\n",
    "    \"arcane_multi_preserve_color.pt\": \"https://www.dropbox.com/s/732gtuvhofyb0t5/arcane_multi_preserve_color.pt?dl=0\",\n",
    "    \"arcane_multi.pt\": \"https://www.dropbox.com/s/fb54ml11u1tyrab/arcane_multi.pt?dl=0\",\n",
    "    \"sketch_multi.pt\": \"https://www.dropbox.com/s/296weom7h43uedj/sketch_multi.pt?dl=0\",\n",
    "    \"disney.pt\": \"https://www.dropbox.com/s/lqt1fcztw6ryp06/disney.pt?dl=0\",\n",
    "    \"disney_preserve_color.pt\": \"https://www.dropbox.com/s/v4kg3tq276ibrjh/disney_preserve_color.pt?dl=0\",\n",
    "    \"jojo.pt\": \"https://www.dropbox.com/s/clx9xvc3pnsd7nl/jojo.pt?dl=0\",\n",
    "    \"jojo_preserve_color.pt\": \"https://www.dropbox.com/s/5mtgon0dldboisf/jojo_preserve_color.pt?dl=0\",\n",
    "    \"jojo_yasuho.pt\": \"https://www.dropbox.com/s/9ckqmpf2kp6x892/jojo_yasuho.pt?dl=0\",\n",
    "    \"jojo_yasuho_preserve_color.pt\": \"https://www.dropbox.com/s/cezm4mbjco97wcx/jojo_yasuho_preserve_color.pt?dl=0\",\n",
    "    \"art.pt\": \"https://www.dropbox.com/s/cj3e77xe6xgxwp5/art.pt?dl=0\",\n",
    "}\n",
    "\n",
    "class Downloader(object):  \n",
    "    def download_file(self, file_name, folder, use_gdown=False, always_download=False):\n",
    "        file_dst = os.path.join(folder, file_name)\n",
    "        file_id = drive_ids[file_name]\n",
    "        if not os.path.exists(file_dst) or always_download:\n",
    "            print(f'Downloading {file_name}') \n",
    "            if use_gdown:\n",
    "              !gdown $file_id -O $file_dst\n",
    "            else:\n",
    "              !wget -nc $file_id -O $file_dst\n",
    "            return False\n",
    "        else:\n",
    "          return True\n",
    "\n",
    "downloader = Downloader()\n",
    "downloader.download_file(\"dlibshape_predictor_68_face_landmarks.dat\",models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ##**Choose Image**\n",
    "\n",
    "#@markdown Choose the character that you want to animate and then run the cell to confirm your selection. \n",
    "\n",
    "#@markdown ![](https://drive.google.com/uc?export=view&id=14hmm9YSvhVGerqRtou3yDGnyzMl_05ry)\n",
    "\n",
    "%cd \"/content\"  \n",
    "character = 'Upload Your Own' #@param [\"Van Gogh\", \"Mona Lisa\", \"Einstein\", \"Lincoln\", \"Nietzsche\", \"Sokrates\", \"Upload Your Own\"]\n",
    "print(f\"{character} selected.\")\n",
    "\n",
    "if character == \"Upload Your Own\":\n",
    "  character_selected = f\"/content/{getLocalFiles()}\"\n",
    "  if character_selected.endswith(\".jpg\"):\n",
    "    Image.open(character_selected).save(character_img)\n",
    "\n",
    "else:\n",
    "  character = character.lower().replace(\" \", \"_\") # make lowercase and remove spacing\n",
    "  downloader.download_file(f'{character}.png',images_dir) \n",
    "  character_selected = f\"{images_dir}/{character}.png\"\n",
    "\n",
    "!cp $character_selected $character_img\n",
    "\n",
    "my_w = None\n",
    "\n",
    "from e4e_projection import projection as e4e_projection ###\n",
    "\n",
    "downloader.download_file('e4e_ffhq_encode.pt',models_dir) ###\n",
    "aligned_face = align_face(character_img)   ###\n",
    "if aligned_face: ###    \n",
    "  name = strip_path_extension(character_img)+'.pt' ###\n",
    "  my_w = e4e_projection(aligned_face, name, 'cuda').unsqueeze(0)   ###\n",
    "\n",
    "display_image(character_img, width=400, height=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ##**Upload/Record Driving Video**\n",
    "#@markdown Your driving video should contain the action that want your character to copy.\n",
    "\n",
    "#@markdown **Important to note that the bigger the face the better the output!**\n",
    "\n",
    "#@markdown Video requirements: \n",
    "#@markdown - ~30 frames per second (otherwise your generated character will move faster/slower)\n",
    "#@markdown - Contain one face in every frame\n",
    "#@markdown - ~30 seconds (Colab might run out of RAM if you upload/record a video that is too long)\n",
    "#@markdown - .MP4 format \n",
    "\n",
    "#@markdown Either record the video using your webcam or upload the video from a file (.mp4). Note that the record function only works on Google Chrome/Microsoft Edge browsers. \n",
    "record_or_upload = \"Record\" #@param [\"Bella Poarch\", \"Record\", \"Upload (.mp4)\"]\n",
    "\n",
    "if record_or_upload == \"Bella Poarch\":\n",
    "  downloader.download_file('driving_video.mp4',folder=\"/content\",always_download=True)\n",
    "\n",
    "elif record_or_upload == \"Record\":\n",
    "    print(\"Please record the video you wish to drive the animation with. Remember to enable your camera and microphone in Chrome:\\n\")\n",
    "    button = widgets.Button(description=\"Record Your Video\") \n",
    "    record_video(driving_video)\n",
    "else:\n",
    "    print(\"Please upload the video you wish to drive the animation with:\\n\")\n",
    "    uploaded = f\"/content/{getLocalFiles()}\"\n",
    "    if uploaded != driving_video:\n",
    "      os.rename(uploaded,driving_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ##**Verify Your Driving Video**\n",
    "#@markdown Ensure that you have uploaded the right video. If you are not satisfied with how it turned out, feel free to return to the previous cell to re-upload/record. \n",
    "\n",
    "display_video(driving_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Style Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_image(character_img):\n",
    "    \"\"\"\n",
    "    Stylize `character_img` (e.g. using JoJoGan) and return the stylized output. \n",
    "    \"\"\"\n",
    "    pretrained = 'art' \n",
    "    preserve_color = False\n",
    "\n",
    "    if preserve_color:\n",
    "        ckpt = f'{pretrained}_preserve_color.pt'\n",
    "    else:\n",
    "        ckpt = f'{pretrained}.pt'\n",
    "\n",
    "    # load base version if preserve_color version not available\n",
    "    try:\n",
    "        downloader.download_file(ckpt)\n",
    "    except:\n",
    "        ckpt = f'{pretrained}.pt'\n",
    "        downloader.download_file(ckpt, models_dir)\n",
    "\n",
    "    generator = Generator(1024, 512, 8, 2).to('cuda')\n",
    "    ckpt = torch.load(os.path.join(models_dir, ckpt),\n",
    "                      map_location=lambda storage, loc: storage)\n",
    "    generator.load_state_dict(ckpt[\"g\"], strict=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generator.eval()\n",
    "        my_sample = generator(my_w, input_is_latent=True)\n",
    "\n",
    "    return my_sample\n",
    "\n",
    "output = style_image(character_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output to the file character_img\n",
    "save_image(utils.make_grid(output, normalize=True, range=(-1, 1)), \n",
    "           save_location=character_img)\n",
    "# display the generated image\n",
    "display_image(character_img, width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Animate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_img(img_src, img_out=trimmed_character_img):\n",
    "    \"\"\"\n",
    "    Trim `img_src` to 400 x 400 pixels centered on a face. The trimmed image will \n",
    "    be saved to trimmed_character_img unless otherwise specified. \n",
    "    \"\"\"\n",
    "    \n",
    "    # read the input image\n",
    "    img = cv2.imread(img_src)\n",
    "    img = imutils.resize(img, width=400)\n",
    "\n",
    "    # convert into grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect where the face is located\n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        '/content/first-order-model/haarcascade_frontalface_alt2.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    try:\n",
    "        for (x, y, w, h) in faces:\n",
    "            extention = 40\n",
    "            faces = img[y-extention:y + h+extention,\n",
    "                        x-extention:x + w + extention]\n",
    "            cv2.imwrite(img_out, faces)\n",
    "    except:\n",
    "        print(\"Error: Face takes too much space on image. Try a different image, or trim it yourself to 400x400.\")\n",
    "    return img_out\n",
    "\n",
    "def animate_video(img_filename, vid_filename, vid_out=animated_video):\n",
    "    \"\"\"\n",
    "    Animate `img_filename` using the video in `vid_filename`. Motions from the \n",
    "    driving video (`vid_filename`) is transfered to object in the static image.\n",
    "    Return the animated frames.  \n",
    "    \"\"\"\n",
    "    # preprocess image to be 400x400\n",
    "    trim_img(img_filename)\n",
    "    downloader.download_file('vox-cpk.pth.tar', models_dir)\n",
    "\n",
    "    %cd /content/first-order-model/\n",
    "\n",
    "    from demo import make_animation\n",
    "    from demo import load_checkpoints\n",
    "    from skimage import img_as_ubyte\n",
    "\n",
    "    # load the image to be animated\n",
    "    source_image = imageio.imread(trimmed_character_img)\n",
    "\n",
    "    # read the driving video at 30 fps\n",
    "    driving_video = skvideo.io.vread(vid_filename, inputdict={'-r': \"30\"})\n",
    "\n",
    "    # resize image and video to 256x256\n",
    "    source_image = resize(source_image, (256, 256))[..., :3]\n",
    "    driving_video = [resize(frame, (256, 256))[..., :3]\n",
    "                     for frame in driving_video]\n",
    "\n",
    "    # Load first order model to transfer motion\n",
    "    generator, kp_detector = load_checkpoints(\n",
    "        config_path='config/vox-256.yaml', checkpoint_path=f\"{models_dir}/vox-cpk.pth.tar\")\n",
    "\n",
    "    # animate source image\n",
    "    predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True,\n",
    "                                 adapt_movement_scale=False)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "animated_frames = animate_video(character_img, driving_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove video if previously generated\n",
    "if os.path.exists(animated_video_no_audio):\n",
    "    os.remove(animated_video_no_audio)\n",
    "    \n",
    "# save resulting video as file named`animated_video_no_audio`\n",
    "imageio.mimsave(animated_video_no_audio, \n",
    "                [img_as_ubyte(frame) for frame in animated_frames], fps=30)\n",
    "\n",
    "# add audio back to video \n",
    "!ffmpeg -y -i $animated_video_no_audio -i $driving_video -c copy -map 0:v:0 -map 1:a:0 -strict -2 $animated_video\n",
    "\n",
    "# add the seetrue logo \n",
    "downloader.download_file('logo_small.png',images_dir)    \n",
    "!ffmpeg -y -i $animated_video -i /content/images/logo_small.png -filter_complex \"overlay=0:H-h\" $final_video\n",
    "\n",
    "# display the generated video \n",
    "display_video(final_video) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Get Creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(final_video)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
